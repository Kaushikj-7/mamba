model_params:
  d_model: 896
  n_layer: 24
  vocab_size: 5
train_params:
  lr: 0.001
  min_lr: 0.0001
  weight_decay: 0.1
