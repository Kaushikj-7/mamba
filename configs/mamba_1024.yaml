model_params:
  d_model: 384
  n_layer: 16
  vocab_size: 5
train_params:
  lr: 0.001
  min_lr: 0.0001
  weight_decay: 0.1
